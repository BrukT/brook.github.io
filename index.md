## Welcome to My page

As you probably know, I am **Bruk Gurmesa**.<be>
I am into Scalable Solutions whether it is big data processing or inference. 

### Cloud Certifications
#### AWS Certified Machine Learning Speciality (MLS-C01)
[https://www.credly.com/badges/ac447329-747e-4fa6-8c23-7306920230b9/public_url](https://www.credly.com/badges/ac447329-747e-4fa6-8c23-7306920230b9/public_url)

#### AWS Certified Developer Associate (DVA-C01) 
[https://www.credly.com/badges/0a7a3ca7-4f42-4ad6-bc9b-148ec19e7e19/public_url](https://www.credly.com/badges/0a7a3ca7-4f42-4ad6-bc9b-148ec19e7e19/public_url)

#### GCP Certified Professional Data Engineer
[https://google.accredible.com/6b6e054d-aba3-40d9-9707-cbcc5b7fde3a](https://google.accredible.com/6b6e054d-aba3-40d9-9707-cbcc5b7fde3a)

### Work Experience

#### [Ford Motor Company](https://www.ford.com/), USA May 2023 – Today
As a Data Engineer, My roles include but not limited to: 
-	Landing, Curating and Publishing various types of IOT data to make it available for various BI Dashboards and ML inferences via Bigquery. Integration of data loss prevention protocols.
-	Migrating bigdata transformation written in Scala to Dataform & Bigquery based workloads on GCP. 
-	as part of an End-to-End team dedicated to real-time anomaly detection, I architected and delivered streaming pipelines using GCP Dataflow, Confluent Kafka, and PubSub. 
-	Investigate, provide, and implement (cost-saving) solutions to reduce cloud storage and processing costs. Help keep the running cost of the team within budget.
- worked on a RAG-based LLM chat application. Specifically Prepared a knowledge base for the LLM Model using text corpse. Implemented a REST protocol-based Access to the LLM chatbot using API Gateway and Lambda function. 
-	Worked on feature engineering preparing ready-to-use features using Astronomer, and Dataflow batch and streaming pipelines.
-	Periodically checking for new bugs in existing dependencies and base container images. Introduce changes and upgrades when vulnerabilities arise.
-	Build and Maintain CICD pipelines using GCP cloud build.


Technologies used: GCP dataflow, Dataproc, bigquery, dataform, cloud build, terraform, github actions, aws api gateway, aws bedrock, spark, apache beam, dbt, airflow, kafka, mqtt

#### [Reply S.p.A](https://www.reply.com/en), Italy November 2020 – April 2023
Reply is an Italian origin international information technology services and consulting company with over 10,500 employees worldwide. Through various reply sister companies I consulted with the following roles. 

As GCP Data Engineer, for client named Intesa Sanpaolo S.p.A, I worked on integrating on-prem data infrastructure with GCP cloud to run data intensive and sporadic ETL processes on the cloud and report result back to on-prem. Design and Mockup testing. 
Tasks Performed:
-	Study the internal Architecture of Intesa Sanpaolo Bank Digital infrastructure. And Propose ways on how to integrate GCP dataproc with the existing architecture.
-	Study and propose how to cost effectively use Dataproc along with GCS and bigquery in the Data hybrid data platform. 

-	Design possible VPC network architectures to create a VPC on GCP that can be accessed from on premises very easily and is used to deploy the ETL Pyspark workload on a Dataproc cluster.
-	GCP Dataproc, GCP Bigquery, GCS, GCP IAM

As a Data Engineer, for Levi Strauss & Co, I worked on building and maintaining data pipelines that deliver quality data to Analysis and Machine Learning models on AWS based Data Lake. Also working on migrating data flows built in Dataiku Data Science studio to standard pipelines orchestrated by in Airflow and ETLed by AWS EMR. At the end I also participated in migrating part of the ETL processes to Feature Store in GCP.
Tasks Performed:
-	Optimize and maintain existing SparkSQL queries to address data quality issues like missing values, data redundancy, schema drifts.
-	Integrated metadata collection process in the ETL process to use the metadata for Data Observability process. Used GCP bigquery and GCP GCS to create tables that is used to store information about the data loaded in cloud storage.
-	Write Jinja2 template which will be used to build YAML descriptions into Airflow pipeline scripts in python. To simplify pipeline preparation process. 
-	Be a scrum master and deploy weekly codes fixes and new features to production. Lead stand-up meetings and deployment discussions.
-	Help data scientists in setting up Jupyter notebook with Dataproc to speed up data exploration process with PySpark. The former exploration tool as pandas and it was taking lots of time as the data gets bigger. 
Technologies Used: Python, Pyspark, SparkSQL, AWS EMR, Apache Airflow, AWS S3, EC2, SSH, Aurora DB, Redshift, Athena, PostgresSQL, Jirra, Dataiku, Databricks, Jupyter Notebook, GCP Dataproc, GCP Bigquery, Apache Superset, SODA DQ, PyDeequ, AWS SageMaker, VertexAI, GCP CE, AWS IAM, Jenkins, GCP Looker Studio

As a Big Data Engineer, for a client named CSE - Banking Services Consortium, I worked on optimizing PySpark scripts implemented to manipulate data stored on Hortonworks Data Platform (HDF) on-premises. After a couple of months, I worked on migrating the existing Data Platform to AWS using AWS native serverless services. 
Tasks Performed:
-	Optimized existing PySpark scripts to run efficiently by utilizing the caching feature in Spark.
-	Propose the architecture to run existing PySpark scripts using AWS Glue including creating python packages to be passed to AWS Glue environment. 
-	Use Glue Data catalog with SparkSQL to utilize existing BigSQL queries in AWS Glue with minimal modifications to address the syntax differences. 
-	wrote AWS Step functions to orchestrate ETL processes on triggers from incoming data on S3 or using signals received by termination of other step functions.
-	Integrated DynamoDB to cache the status of each ETL process.
-	Setup a reporting process using AWS lambda function to get the status of the ETL processes from DynamoDB and send it to on-premises via SFTP protocol.
Technologies Used: Python, Pyspark, SparkSQL, AWS Glue, SFTP protocol, AWS Lambda, EC2, SSH, AWS StepFunction, Dynamo DB, Aurora DB, Athena, PostgresSQL, Jirra


#### Hawassa University:
Position: Teaching Assistant <br />
Role: Giving Tutorials and Labs, Assisting Research  <br />
Duration: 09/2016 - 09/2018 <br />

#### Icog_Labs: 
position: full time paid intern <br />
role: Software Programmer <br />
duration: 07/2015 – 01/2016

### Education
#### University of Pisa:  
MSc in Computer Engineering: 10/2018 - 11/2020 <br />
My Thesis on: <br />
* **Integration of CoAP-based sensors into the KubeEdge Fog/Edge computing platform** <br />
[abstract page](https://etd.adm.unipi.it/t/etd-10152020-211154)

coding with: 
* **Go** programming language

#### Hawassa University: 
BSc in Electrical and Computer Engineering: 10/2011-07/2016

#### Online Courses:
Provider: coursera.org <br>
courses: 
* Data Science with R, 
* Web development: JavaScript and Bootstrap
* Machine Learning: Octave 
* ...

*certificates are attached at my linkedin profile [page](https://www.linkedin.com/in/brukt/)*

### Contact


[Linkedin Profile](https://www.linkedin.com/in/brukt/)
<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>
